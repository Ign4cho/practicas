{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import gspread as gs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ejecutar para crear un nuevo archivo en google drive la primera vez\n",
    "\n",
    "# sh = gc.create('documentos_procesado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del archivo .xlsx\n",
    "file_path = \"revision_documentos.xlsx\"\n",
    "file_path_destino = \"revision_documentos_procesado_\" + datetime.now().strftime(\"%Y%m%d%H%M\") + \".xlsx\"\n",
    "credentials_filename='credenciales\\client_secret_290615798724-sg1nk4latali6a8akhe169r66gg7hmd8.apps.googleusercontent.com.json'\n",
    "\n",
    "# Fecha de inicio y fin de la revisión\n",
    "\n",
    "fecha_inicial = datetime(2024, 8, 1)\n",
    "fecha_final = datetime(2024, 8, 10)\n",
    "\n",
    "# Lee el archivo Excel\n",
    "df = pd.read_excel(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# si importamos directamente desde google drive\n",
    "credentials_filename='credenciales\\client_secret_290615798724-sg1nk4latali6a8akhe169r66gg7hmd8.apps.googleusercontent.com.json'\n",
    "gc = gs.oauth(credentials_filename)\n",
    "sh = gc.open('documentos_procesado')\n",
    "worksheet = sh.sheet1\n",
    "df = pd.DataFrame(worksheet.get_all_records())\n",
    "\n",
    "# Fecha de inicio y fin de la revisión\n",
    "\n",
    "fecha_inicial = datetime(2024, 8, 1)\n",
    "fecha_final = datetime(2024, 8, 10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['DOCUMENTOVIGENCIA', 'FECHA_INICIO_DOCUMENTO', 'FECHA_FIN_DOCUMENTO', 'DOCO_EMPR_RUT', 'Hora extendida', 'Fecha y hora', 'Fecha.1', 'Gestionado', 'Tiempo desde ingreso', 'Tiempo para cumplir en plazo', 'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43', 'Unnamed: 29', 'Unnamed: 30', 'Fecha', 'Hora', 'AM/PM'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Para el primer analisis, se eliminan las columnas que no son relevantes\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# se eliminan las columnas: DOCUMENTOVIGENCIA, FECHA_INICIO_DOCUMENTO, FECHA_FIN_DOCUMENTO\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# DOCO_EMPR_RUT, TIPO, 'Hora extendida', 'Fecha y hora', 'Fecha.1', 'Gestionado','Tiempo desde ingreso', 'Tiempo para cumplir en plazo', 'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43' 'Unnamed: 29','Unnamed: 30', 'Fecha', 'Hora', 'AM/PM'\u001b[39;00m\n\u001b[0;32m      4\u001b[0m columnas_no_tenidas_en_cuenta \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOCUMENTOVIGENCIA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFECHA_INICIO_DOCUMENTO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFECHA_FIN_DOCUMENTO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOCO_EMPR_RUT\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTIPO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHora extendida\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFecha y hora\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFecha.1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGestionado\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTiempo desde ingreso\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTiempo para cumplir en plazo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 40\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 41\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 42\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 43\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 29\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 30\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFecha\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHora\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAM/PM\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumnas_no_tenidas_en_cuenta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#verificamos cuantos usuarioregistroid son nulos\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSUARIOREGISTROID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5112\u001b[0m     labels: IndexLabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5122\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5123\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5256\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5260\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5264\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5265\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5266\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4547\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4549\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4589\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4591\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4592\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4594\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\indexes\\base.py:6699\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6699\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6700\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['DOCUMENTOVIGENCIA', 'FECHA_INICIO_DOCUMENTO', 'FECHA_FIN_DOCUMENTO', 'DOCO_EMPR_RUT', 'Hora extendida', 'Fecha y hora', 'Fecha.1', 'Gestionado', 'Tiempo desde ingreso', 'Tiempo para cumplir en plazo', 'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43', 'Unnamed: 29', 'Unnamed: 30', 'Fecha', 'Hora', 'AM/PM'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Para el primer analisis, se eliminan las columnas que no son relevantes\n",
    "# se eliminan las columnas: DOCUMENTOVIGENCIA, FECHA_INICIO_DOCUMENTO, FECHA_FIN_DOCUMENTO\n",
    "# DOCO_EMPR_RUT, TIPO, 'Hora extendida', 'Fecha y hora', 'Fecha.1', 'Gestionado','Tiempo desde ingreso', 'Tiempo para cumplir en plazo', 'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43' 'Unnamed: 29','Unnamed: 30', 'Fecha', 'Hora', 'AM/PM'\n",
    "columnas_no_tenidas_en_cuenta = [\n",
    "    'DOCUMENTOVIGENCIA', 'FECHA_INICIO_DOCUMENTO', 'FECHA_FIN_DOCUMENTO', 'DOCO_EMPR_RUT',\n",
    "    'TIPO', 'Hora extendida', 'Fecha y hora', 'Fecha.1', 'Gestionado',\n",
    "     'Tiempo desde ingreso', 'Tiempo para cumplir en plazo', 'Unnamed: 40',\n",
    "     'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43', 'Unnamed: 29','Unnamed: 30',\n",
    "     'Fecha', 'Hora', 'AM/PM']\n",
    "\n",
    "df.drop(columnas_no_tenidas_en_cuenta,\n",
    "        axis=1, inplace=True)\n",
    "\n",
    "#verificamos cuantos usuarioregistroid son nulos\n",
    "print(df['USUARIOREGISTROID'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la columna tipo, en base a la columna estado. \n",
    "# Si el estado es \"Enviado\", el tipo es de \"Entrada\"\n",
    "# Si el estado es \"Rechazado\" o \"Validado\", el tipo es de \"Salida\"\n",
    "\n",
    "df[\"TIPO\"] = df[\"ESTADO\"].apply(lambda x: \"Entrada\" if x == \"Enviado\" else \"Salida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos la columna FECHA_REGISTRO_HH, en formato mm/dd/yyyy hh:mm:ss PM/AM a datetime\n",
    "df[\"FECHA_REGISTRO_HH\"] = pd.to_datetime(df[\"FECHA_REGISTRO_HH\"], format=\"%m/%d/%Y %I:%M:%S %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos dos dataframes, uno con los documentos de entrada y otro con los de salida\n",
    "df_entrada = df[df[\"TIPO\"] == \"Entrada\"]\n",
    "df_salida = df[df[\"TIPO\"] == \"Salida\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_entrada.shape)\n",
    "print(df_salida.shape)\n",
    "\n",
    "columnas_eliminadas_salida = ['FECHA_REGISTRO', 'ORIGENDOCID', 'ORIGEN',\n",
    "       'DOCO_TIDO_ID', 'ID_ESTADO',\n",
    "       'DOCUMENTOSITUACIONID', 'DOCUMENTOSITUCION', 'USUARIOREGISTRO',\n",
    "       'USUARIOREGISTROCUENTA', 'USUARIOREGISTRONOMBRE',\n",
    "       'DOCO_TIPO_VALIDACION', 'DOCO_CONT_ID', 'FECHA_INGRESO_DOCUMENTO'\n",
    "       , 'DOCUMENTOESPECIALIDADID', 'DOCUMENTOESPECIALIDAD',\n",
    "       'DOCUMENTOOCUPACIONID', 'DOCUMENTOOCUPACION',\n",
    "       'DOCUMENTOCERTOCUPACIONID', 'DOCUMENTOCERTOCUPACION', 'TIPO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del dataframe de salidas, nos interesa conservar las columnas\n",
    "# FECHA_REGISTRO_HH, ESTADO, USUARIOREGISTROID, pero renombramos a \n",
    "# FECHA_SALIDA, ESTADO_SALIDA, USUARIOID_SALIDA  para luego poder hacer el merge\n",
    "df_salida = df_salida.rename(columns={\"FECHA_REGISTRO_HH\": \"FECHA_SALIDA\", \"ESTADO\": \"ESTADO_SALIDA\", \"USUARIOREGISTROID\": \"USUARIOID_SALIDA\"})\n",
    "df_salida.drop(\n",
    "       columnas_eliminadas_salida\n",
    "       , axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacemos el merge, en base al doco_id, lo hacemos de tipo left para conservar los documentos de entrada\n",
    "df_merged = pd.merge(df_entrada, df_salida, on=\"DOCO_ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código de abajo es para ver si hay documentos que tienen salida pero no entrada, que en teoría no debería ocurrir\n",
    "Una de las causas puede ser que la entrada este en otro documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos el merge de tipo right para conservar todos los documentos de salida\n",
    "df_right_merged = pd.merge(df_entrada, df_salida, on=\"DOCO_ID\", how=\"right\")\n",
    "\n",
    "# Filtramos los registros donde ID_ESTADO de entrada sea NaN\n",
    "df_salida_sin_entrada = df_right_merged[df_right_merged['ID_ESTADO'].isna()]\n",
    "\n",
    "# Mostramos los documentos de salida que no tienen un documento de entrada correspondiente\n",
    "# print(df_salida_sin_entrada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agregamos una columna con la diferencia de tiempo entre entrada y salida de los documentos en horas \n",
    "# en caso de no haber fecha salida, la columna se rrrellena con NaN\n",
    "# en caso de no haber fecha de registro, la columna se rellena con NaN\n",
    "\n",
    "df_merged[\"DIFERENCIA_HORAS\"] = df_merged.apply(\n",
    "    lambda row: (row[\"FECHA_SALIDA\"] - row[\"FECHA_REGISTRO_HH\"]).total_seconds() / 3600\n",
    "    if pd.notnull(row[\"FECHA_SALIDA\"]) and pd.notnull(row[\"FECHA_REGISTRO_HH\"]) else np.nan,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el umbral de 48 horas\n",
    "umbral = 48\n",
    "\n",
    "# Agregamos un campo de desviación respecto al umbral\n",
    "df_merged[\"DESVIACION\"] = df_merged[\"DIFERENCIA_HORAS\"] - umbral\n",
    "\n",
    "# Determinamos el estado, teniendo en cuenta la desviación y los documentos\n",
    "# sin salida\n",
    "\n",
    "df_merged['estado'] = df_merged.apply(\n",
    "    lambda row: 'Sin gestionar' if pd.isna(row['FECHA_SALIDA'])\n",
    "    else 'Sin entrada' if pd.isna(row['FECHA_REGISTRO_HH'])\n",
    "    else 'Fuera de plazo' if row['DESVIACION'] > 0\n",
    "    else 'En plazo' if row['DESVIACION'] < 0\n",
    "    else 'Igual', \n",
    "    axis=1\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el mes actual y los tres meses anteriores\n",
    "hoy = datetime.now()\n",
    "mes_actual = hoy.month\n",
    "año_actual = hoy.year\n",
    "\n",
    "# Crear una lista de los tres meses anteriores y el mes actual\n",
    "meses_validos = [(mes_actual - i - 1) % 12 + 1 for i in range(4)]\n",
    "año_valido = año_actual if mes_actual in meses_validos else año_actual - 1\n",
    "\n",
    "# Función para obtener la etiqueta del mes\n",
    "def etiqueta_mes(fecha):\n",
    "    mes = fecha.month\n",
    "    año = fecha.year\n",
    "    \n",
    "    if mes == mes_actual and año == año_actual:\n",
    "        return f\"{hoy.strftime('%B')} {año_actual}\"\n",
    "    \n",
    "    if mes in meses_validos:\n",
    "        return f\"{fecha.strftime('%B')} {año}\"\n",
    "    \n",
    "    return 'Fuera de rango'\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "df_merged['etiqueta_mes'] = df_merged['FECHA_REGISTRO_HH'].apply(etiqueta_mes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos del DataFrame a un archivo Excel\n",
    "#df_merged.to_excel(file_path_destino, index=False)\n",
    "\n",
    "# TODO sacar comentario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtramos el dataframe en por fecha de salida \n",
    "\n",
    "df_merged = df_merged[(df_merged['FECHA_SALIDA'] >= fecha_inicial) & (df_merged['FECHA_SALIDA'] <= fecha_final)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparamos la exprotación a sheets\n",
    "#  es necesario convertir los timestamps a string, en formato yyyy-mm-dd hh:mm:ss\n",
    "# convertimos los timestamps a string, teniendo en cuenta que pueden haber NaTs\n",
    "\n",
    "df_merged = df_merged.applymap(\n",
    "    lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    if isinstance(x, (pd.Timestamp, datetime)) and not pd.isnull(x) else x\n",
    ")\n",
    "\n",
    "# reemplazamos NaNs con espccios vacíos\n",
    "df_merged= df_merged.applymap(lambda x: '' if pd.isnull(x) else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = gs.oauth(credentials_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportamos a google sheets\n",
    "\n",
    "sh = gc.open('documentos_procesado')\n",
    "worksheet = sh.sheet1\n",
    "worksheet.update([df_merged.columns.values.tolist()] + df_merged.values.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
